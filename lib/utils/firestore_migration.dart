import 'package:cloud_firestore/cloud_firestore.dart';
import 'text_normalizer.dart';
import 'logger.dart';

/// Utilitaire pour migrer et nettoyer les doublons dans Firestore
class FirestoreMigration {
  static final FirebaseFirestore _firestore = FirebaseFirestore.instance;
  static const String _programmesCollection = 'programme';
  static const String _coursesCollection = 'cours';

  /// Migre et nettoie tous les doublons de programmes
  static Future<void> migrateProgrammes() async {
    Logger.info('üßπ D√©but migration programmes - nettoyage des doublons');
    
    try {
      // 1. R√©cup√©rer tous les programmes existants
      final snapshot = await _firestore
          .collection(_programmesCollection)
          .get();

      Map<String, List<DocumentSnapshot>> programmeGroups = {};
      
      // 2. Grouper par mati√®re/niveau/ann√©e normalis√©s
      for (var doc in snapshot.docs) {
        final data = doc.data();
        final matiere = data['matiere'] as String? ?? '';
        final niveau = data['niveau'] as String? ?? '';
        final annee = data['annee'] as int? ?? DateTime.now().year;
        
        // G√©n√©rer la cl√© normalis√©e
        final normalizedMatiere = TextNormalizer.normalizeMatiere(matiere);
        final normalizedNiveau = TextNormalizer.normalizeNiveau(niveau);
        final normalizedKey = '${normalizedMatiere}_${normalizedNiveau}_$annee';
        
        if (!programmeGroups.containsKey(normalizedKey)) {
          programmeGroups[normalizedKey] = [];
        }
        programmeGroups[normalizedKey]!.add(doc);
      }

      // 3. Traiter chaque groupe
      int mergedCount = 0;
      int deletedCount = 0;
      
      for (var entry in programmeGroups.entries) {
        final key = entry.key;
        final docs = entry.value;
        
        if (docs.length > 1) {
          Logger.info('üìã Trouv√© ${docs.length} doublons pour: $key');
          
          // Garder le plus r√©cent et fusionner les contenus
          await _mergeProgrammes(docs);
          mergedCount++;
          deletedCount += docs.length - 1;
        }
      }
      
      Logger.info('‚úÖ Migration termin√©e: $mergedCount groupes fusionn√©s, $deletedCount doublons supprim√©s');
      
    } catch (e) {
      Logger.error('‚ùå Erreur migration programmes: $e');
    }
  }

  /// Fusionne plusieurs programmes doublons en un seul
  static Future<void> _mergeProgrammes(List<DocumentSnapshot> docs) async {
    try {
      // Trier par date de cr√©ation (le plus r√©cent en premier)
      docs.sort((a, b) {
        final aData = a.data() as Map<String, dynamic>;
        final bData = b.data() as Map<String, dynamic>;
        
        final aCreated = (aData['createdAt'] as Timestamp?)?.toDate() ?? DateTime.now();
        final bCreated = (bData['createdAt'] as Timestamp?)?.toDate() ?? DateTime.now();
        
        return bCreated.compareTo(aCreated); // Plus r√©cent en premier
      });

      final mainDoc = docs.first;
      final mainData = mainDoc.data() as Map<String, dynamic>;
      
      // Normaliser les donn√©es du document principal
      final normalizedMatiere = TextNormalizer.normalizeMatiere(mainData['matiere'] as String? ?? '');
      final normalizedNiveau = TextNormalizer.normalizeNiveau(mainData['niveau'] as String? ?? '');
      
      // Fusionner les chapitres de tous les documents
      Set<String> allChapitres = {};
      String? bestContenu = mainData['contenu'] as String?;
      
      for (var doc in docs) {
        final data = doc.data() as Map<String, dynamic>;
        final chapitres = (data['chapitres'] as List?)?.cast<String>() ?? [];
        allChapitres.addAll(chapitres);
        
        // Garder le contenu le plus long/d√©taill√©
        final contenu = data['contenu'] as String?;
        if (contenu != null && (bestContenu == null || contenu.length > bestContenu.length)) {
          bestContenu = contenu;
        }
      }

      // Cr√©er le document fusionn√© avec des donn√©es normalis√©es
      final mergedData = {
        ...mainData,
        'matiere': normalizedMatiere,
        'niveau': normalizedNiveau,
        'chapitres': allChapitres.toList(),
        'contenu': bestContenu,
        'updatedAt': FieldValue.serverTimestamp(),
        'metadata': {
          ...((mainData['metadata'] as Map<String, dynamic>?) ?? {}),
          'merged_from': docs.map((d) => d.id).toList(),
          'merged_at': DateTime.now().toIso8601String(),
        },
      };

      // Cr√©er nouveau document avec ID normalis√©
      final normalizedId = _generateNormalizedId(normalizedMatiere, normalizedNiveau, mainData['annee'] as int? ?? DateTime.now().year);
      
      await _firestore
          .collection(_programmesCollection)
          .doc(normalizedId)
          .set(mergedData);

      // Supprimer tous les anciens documents
      final batch = _firestore.batch();
      for (var doc in docs) {
        batch.delete(doc.reference);
      }
      await batch.commit();
      
      Logger.info('üîÑ Fusionn√© ${docs.length} programmes -> $normalizedId');
      
    } catch (e) {
      Logger.error('‚ùå Erreur fusion programmes: $e');
    }
  }

  /// G√©n√®re un ID normalis√© pour √©viter les doublons futurs
  static String _generateNormalizedId(String matiere, String niveau, int annee) {
    final normalizedMatiere = matiere.toLowerCase()
        .replaceAll(' ', '_')
        .replaceAll('-', '_')
        .replaceAll('√©', 'e')
        .replaceAll('√®', 'e')
        .replaceAll('√ß', 'c');
    
    final normalizedNiveau = niveau.toLowerCase()
        .replaceAll('√®', 'e')
        .replaceAll('√©', 'e')
        .replaceAll(' ', '_');
    
    return '${normalizedMatiere}_${normalizedNiveau}_$annee';
  }

  /// Migre et nettoie tous les doublons de cours
  static Future<void> migrateCourses() async {
    Logger.info('üßπ D√©but migration cours - nettoyage des doublons');
    
    try {
      final snapshot = await _firestore
          .collection(_coursesCollection)
          .get();

      Map<String, List<DocumentSnapshot>> courseGroups = {};
      
      for (var doc in snapshot.docs) {
        final data = doc.data();
        final title = data['title'] as String? ?? '';
        final matiere = data['matiere'] as String? ?? '';
        final niveau = data['niveau'] as String? ?? '';
        final type = data['type'] as String? ?? '';
        
        final normalizedMatiere = TextNormalizer.normalizeMatiere(matiere);
        final normalizedNiveau = TextNormalizer.normalizeNiveau(niveau);
        final normalizedKey = '${title.toLowerCase()}_${normalizedMatiere}_${normalizedNiveau}_$type';
        
        if (!courseGroups.containsKey(normalizedKey)) {
          courseGroups[normalizedKey] = [];
        }
        courseGroups[normalizedKey]!.add(doc);
      }

      int mergedCount = 0;
      int deletedCount = 0;
      
      for (var entry in courseGroups.entries) {
        final docs = entry.value;
        
        if (docs.length > 1) {
          Logger.info('üìö Trouv√© ${docs.length} cours doublons pour: ${entry.key}');
          await _mergeCourses(docs);
          mergedCount++;
          deletedCount += docs.length - 1;
        }
      }
      
      Logger.info('‚úÖ Migration cours termin√©e: $mergedCount groupes fusionn√©s, $deletedCount doublons supprim√©s');
      
    } catch (e) {
      Logger.error('‚ùå Erreur migration cours: $e');
    }
  }

  /// Fusionne plusieurs cours doublons
  static Future<void> _mergeCourses(List<DocumentSnapshot> docs) async {
    try {
      docs.sort((a, b) {
        final aData = a.data() as Map<String, dynamic>;
        final bData = b.data() as Map<String, dynamic>;
        
        final aCreated = (aData['createdAt'] as Timestamp?)?.toDate() ?? DateTime.now();
        final bCreated = (bData['createdAt'] as Timestamp?)?.toDate() ?? DateTime.now();
        
        return bCreated.compareTo(aCreated);
      });

      final mainDoc = docs.first;
      final mainData = mainDoc.data() as Map<String, dynamic>;
      
      final normalizedMatiere = TextNormalizer.normalizeMatiere(mainData['matiere'] as String? ?? '');
      final normalizedNiveau = TextNormalizer.normalizeNiveau(mainData['niveau'] as String? ?? '');
      
      String? bestContent = mainData['content'] as String?;
      
      for (var doc in docs) {
        final data = doc.data() as Map<String, dynamic>;
        final content = data['content'] as String?;
        if (content != null && (bestContent == null || content.length > bestContent.length)) {
          bestContent = content;
        }
      }

      final mergedData = {
        ...mainData,
        'matiere': normalizedMatiere,
        'niveau': normalizedNiveau,
        'content': bestContent,
        'updatedAt': FieldValue.serverTimestamp(),
        'metadata': {
          ...((mainData['metadata'] as Map<String, dynamic>?) ?? {}),
          'merged_from': docs.map((d) => d.id).toList(),
          'merged_at': DateTime.now().toIso8601String(),
        },
      };

      await _firestore
          .collection(_coursesCollection)
          .doc(mainDoc.id)
          .set(mergedData);

      final batch = _firestore.batch();
      for (var doc in docs.skip(1)) {
        batch.delete(doc.reference);
      }
      await batch.commit();
      
      Logger.info('üîÑ Fusionn√© ${docs.length} cours -> ${mainDoc.id}');
      
    } catch (e) {
      Logger.error('‚ùå Erreur fusion cours: $e');
    }
  }

  /// Lance la migration compl√®te
  static Future<void> runFullMigration() async {
    Logger.info('üöÄ D√©but migration compl√®te Firestore');
    
    await migrateProgrammes();
    await migrateCourses();
    
    Logger.info('üéâ Migration compl√®te termin√©e !');
  }
}